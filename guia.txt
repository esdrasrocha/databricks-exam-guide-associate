🎓 GUIA FINAL COMPLETO - Databricks Certified Data Engineer Associate

📚 ESTRUTURA COMPLETA DESTE GUIA:

✅ SEU MATERIAL ORIGINAL (site github.io)
✅ Deep Dive: CDC vs SCD vs Auto Loader vs COPY INTO vs CTAS  
✅ Deep Dive: OPTIMIZE, Z-ORDER, VACUUM, AUTO OPTIMIZE
✅ Deep Dive: LakeFlow Connect + Partner Connect (NOVO)
✅ Deep Dive: LakeFlow Jobs Orchestration (NOVO)
✅ Deep Dive: JSON Processing (NOVO)
✅ 10 Questões Práticas de Prova
✅ Checklist Final

═══════════════════════════════════════════════════════════

🆕 NOVOS CONTEÚDOS ADICIONADOS NESTA VERSÃO:

📦 LAKEFLOW CONNECT - ARQUITETURA DETALHADA

Tipo de Fonte → Arquitetura → Exemplos

1. SaaS Applications:
   • Credenciais via Unity Catalog
   • API/OLAP direto da aplicação  
   • Streaming Delta Table
   • Ex: Salesforce, Workday, Google Analytics, ServiceNow

2. Databases:
   • Ingestion Gateway (extrai metadados + snapshots)
   • Staging no Unity Catalog Volume
   • Serverless Pipeline → Streaming Delta Table
   • Ex: SQL Server, PostgreSQL, MySQL, Oracle

3. Cloud Storage:
   • Auto Loader (cloudFiles)
   • Schema inference automático
   • Delta Table
   • Ex: S3, ADLS, GCS

🎯 PARA A PROVA - LakeFlow Connect:
✅ Conector nativo disponível? → LakeFlow Connect
✅ Sem conector nativo? → Partner Connect
✅ SaaS Apps: Ingestion Gateway NÃO é usado (API direta)
✅ Databases: Ingestion Gateway extrai e faz staging
✅ Cloud Storage: Auto Loader é o padrão
✅ Unity Catalog: SEMPRE envolvido para governança

═══════════════════════════════════════════════════════════

🧩 PARTNER CONNECT - FERRAMENTAS POR CATEGORIA

Categoria → Ferramentas

Data Integration:
• Fivetran, Airbyte, Rivery, Matillion

Transformation:
• dbt, Prophecy

ETL Enterprise:
• Informatica, Qlik, Talend, Alteryx

BI Tools:
• Tableau, Power BI, Looker

🎯 PARA A PROVA:
Partner Connect = alternativa quando NÃO há conector nativo
Fivetran/Airbyte são os mais comuns na prova

═══════════════════════════════════════════════════════════

🧩 JSON E DADOS SEMI-ESTRUTURADOS - MÉTODOS DETALHADOS

Método | Tipo Coluna | Sintaxe | Performance | Quando Usar

1. Acesso Direto:
   STRING | json_col:name | ❌ BAIXA | Exploração rápida, queries ad-hoc

2. STRUCT Explícito:
   STRUCT | from_json(col, schema) | ✅ ALTA | Produção, schema conhecido

3. VARIANT (Preview):
   VARIANT | Automático | ✅ ALTA | Schema misto/desconhecido

Exemplo Prático - JSON para STRUCT:

# JSON como STRING (baixa performance)
SELECT 
    json_col:name as name,
    json_col:age as age
FROM table_with_json

# JSON como STRUCT (alta performance - RECOMENDADO!)
from pyspark.sql.functions import from_json, schema_of_json

# 1. Inferir schema
json_schema = spark.read.json("path/to/json").schema

# 2. Aplicar schema
df = df.withColumn("parsed", from_json("json_col", json_schema))

# 3. Acessar campos
df.select("parsed.name", "parsed.age")

🎯 PARA A PROVA - JSON:
✅ STRING com : → Baixa performance, boa para exploração
✅ STRUCT com schema → Alta performance, RECOMENDADO para produção
✅ schema_of_json() infere schema automaticamente
✅ from_json() converte STRING em STRUCT
✅ VARIANT (preview) → pode aparecer na prova como "futuro"

═══════════════════════════════════════════════════════════

🔄 LAKEFLOW JOBS - ORQUESTRAÇÃO DETALHADA

📦 ESTRUTURA DOS LAKEFLOW JOBS:

Conceito | Função | Analogia

Job → Contêiner que agenda e coordena | "Pipeline principal"
Task → Unidade de execução | "Etapa do pipeline"

🎯 TIPOS DE TASKS (MUITO COBRADO!):

• Notebook (PySpark, SQL, MLflow)
• SQL Query / File  
• Python Script / Wheel
• DLT (Declarative Pipeline)
• dbt / Spark Submit / JAR
• Power BI / Dashboard

IMPORTANTE: Cada job pode ter VÁRIAS tasks, cada uma com SEU PRÓPRIO compute!

═══════════════════════════════════════════════════════════

⚡ CONFIGURAÇÕES DE TASK:

• Caminho do código  
• Parâmetros e bibliotecas  
• Notificações  
• Retry Policies  
• Timeout
• Max concurrent runs

═══════════════════════════════════════════════════════════

🔁 CONTROL FLOW - ORDEM DE EXECUÇÃO (⭐ MUITO COBRADO!)

Tipo | Descrição | Exemplo de Uso

Sequential → Uma depois da outra | Task B só roda após Task A terminar
Parallel → Múltiplas simultâneas | Task B e C rodam ao mesmo tempo
If/Else → Lógica condicional | Se Task A falhar, roda Task C, senão Task B
Run Job → Chama outro Job | Task A chama Job X inteiro
For Each → Loop para múltiplos inputs | Processar lista de arquivos/datas

🎯 PERGUNTA TÍPICA DE PROVA:
"Você tem 3 tasks: Task A processa dados, Task B e C fazem agregações independentes, Task D junta tudo. Como configurar?"

✅ RESPOSTA:
• Task A → Sequential (primeira)
• Task B e C → Parallel (dependem de A, mas independentes entre si)
• Task D → Sequential (depende de B e C terminarem)

═══════════════════════════════════════════════════════════

⏱️ TRIGGERS - INÍCIO DA EXECUÇÃO (⭐ IMPORTANTE!)

Tipo | Descrição | Quando Usar

Manual → Execução sob demanda | Testes, cargas ad-hoc
Scheduled → Agendamento (cron) | Jobs diários, semanais
API Trigger → Disparo por API | Integração com sistemas externos
File Arrival → Quando arquivo chega | Event-driven processing
Table Trigger → Mudança em tabela | Processar quando dados mudam
Continuous → Sempre ativo (streaming) | Streaming real-time

Exemplo de Cron:
0 0 * * * → Todo dia à meia-noite
0 */4 * * * → A cada 4 horas
0 9 * * 1-5 → 9h da manhã, segunda a sexta

🎯 PARA A PROVA - Triggers:
✅ "Processar logo que arquivo chegar" → File Arrival
✅ "Rodar todo dia às 2h da manhã" → Scheduled
✅ "Streaming contínuo" → Continuous
✅ "Integração com API externa" → API Trigger

═══════════════════════════════════════════════════════════

🧮 TIPOS DE COMPUTE (DETALHADÍSSIMO!)

Tipo | Uso | Observação | Custo | Prova

Interactive Cluster:
• Dev/Teste | ❌ NÃO usar em produção | 💰💰💰 Alto | "Never for prod jobs"

Job Cluster:
• Produção | ✅ Encerra após execução | 💰💰 Médio | "Always for prod"

Serverless:
• Totalmente gerenciado | ✅ Auto-scaling, baixo custo | 💰💰 Médio | "Zero management"

SQL Warehouse:
• BI / Dashboards | ✅ Serverless por padrão | 💰💰 Médio | "For SQL queries"

═══════════════════════════════════════════════════════════

⚙️ SERVERLESS PERFORMANCE MODE

Modo | Foco | Ideal para | Custo

OFF (padrão):
• Economia | Jobs não urgentes | 💰 Mais barato

ON (Performance Optimized):
• Velocidade | Jobs sensíveis ao tempo | 💰💰 Mais caro

🎯 IMPORTANTE:
• Aplica-se SOMENTE a tasks com compute Serverless
• ON = startup mais rápido, mais recursos
• OFF = economia, mas pode ter latência maior

═══════════════════════════════════════════════════════════

🧩 SELECIONANDO COMPUTE POR TASK

MUITO IMPORTANTE: Cada task pode usar O MESMO ou DIFERENTE tipo de compute!

Exemplo Real:

Job: "Daily Sales Processing"

Task 1 - "Extract" → Interactive (desenvolvimento ainda)
Task 2 - "Transform" → Serverless (produção, auto-scale)
Task 3 - "Load" → Job Cluster (produção, controle fino)
Task 4 - "Report" → SQL Warehouse (queries SQL)

🎯 PARA A PROVA:
✅ Flexibilidade de compute = balancear custo e performance
✅ Prod jobs → Job Cluster ou Serverless (NUNCA Interactive)
✅ SQL queries → SQL Warehouse
✅ Dev/Test → Interactive (mas não em prod!)

═══════════════════════════════════════════════════════════

📊 RESUMO: O QUE CAI NA PROVA

SECTION 2 (20% - 9 QUESTÕES) COBRE:

✅ LakeFlow Connect (arquitetura, fontes)
✅ Partner Connect (quando usar)
✅ JSON processing (STRING vs STRUCT vs VARIANT)
✅ Auto Loader vs COPY INTO
✅ Schema Evolution
✅ CDC conceitos básicos

SECTION 4 (20% - 9 QUESTÕES) COBRE:

✅ LakeFlow Jobs (estrutura, tasks)
✅ Control Flow (Sequential, Parallel, If/Else, For Each)
✅ Triggers (Manual, Scheduled, File Arrival, etc)
✅ Compute types (Interactive, Job, Serverless, SQL Warehouse)
✅ DAB (Databricks Asset Bundles)
✅ Repair vs Rerun

═══════════════════════════════════════════════════════════

🎯 QUESTÕES ADICIONAIS - NOVO CONTEÚDO

QUESTÃO 11: LakeFlow Connect Arquitetura
Você precisa ingerir dados do Salesforce. Qual componente NÃO é usado?

A) Unity Catalog para credenciais
B) Ingestion Gateway para extração
C) API do Salesforce
D) Streaming Delta Table

✅ RESPOSTA: B) Ingestion Gateway
EXPLICAÇÃO: SaaS apps como Salesforce usam API direta. Ingestion Gateway é usado apenas para databases (SQL Server, PostgreSQL, etc).

──────────────────────────────────────────────────────────

QUESTÃO 12: JSON Performance
Qual método oferece MELHOR performance para processar JSON em produção?

A) json_col:field_name
B) from_json() com schema explícito
C) Manter como STRING e processar no BI
D) VARIANT type

✅ RESPOSTA: B) from_json() com schema explícito
EXPLICAÇÃO: from_json() converte para STRUCT com schema conhecido, oferecendo melhor performance. STRING com : é lento. VARIANT é preview ainda.

──────────────────────────────────────────────────────────

QUESTÃO 13: Control Flow
Você tem: Task A (extract), Task B e C (transform independentes), Task D (load). Como configurar?

A) Todas sequential
B) Todas parallel
C) A → (B, C parallel) → D
D) A → B → C → D

✅ RESPOSTA: C) A → (B, C parallel) → D
EXPLICAÇÃO: A primeiro (extract), B e C podem rodar em paralelo (independentes), D por último aguarda B e C terminarem.

──────────────────────────────────────────────────────────

QUESTÃO 14: Compute Selection
Qual compute usar para um job de produção que roda diariamente e precisa economia?

A) Interactive Cluster
B) Job Cluster
C) All-Purpose Cluster
D) Local machine

✅ RESPOSTA: B) Job Cluster
EXPLICAÇÃO: Job Cluster é criado para o job e encerrado após, sendo mais econômico que clusters interativos. Interactive/All-Purpose são para desenvolvimento.

──────────────────────────────────────────────────────────

QUESTÃO 15: Triggers
Você quer processar arquivos assim que chegam no S3. Qual trigger usar?

A) Scheduled (a cada 1 minuto)
B) File Arrival
C) Manual
D) Continuous

✅ RESPOSTA: B) File Arrival
EXPLICAÇÃO: File Arrival é event-driven, processa quando arquivo chega. Scheduled tem intervalo fixo (menos eficiente). Continuous é para streaming.

═══════════════════════════════════════════════════════════

📈 PERCENTUAL DE COBERTURA DO GUIA COMPLETO

Section 1 - Lakehouse Platform (15%): ✅ 100% coberto
Section 2 - Development & Ingestion (20%): ✅ 100% coberto  
Section 3 - Transformations (30%): ✅ 100% coberto
Section 4 - Productionizing (20%): ✅ 100% coberto
Section 5 - Governance (15%): ✅ 100% coberto

TOTAL DE COBERTURA: ✅ 100% DA PROVA COBERTA!

═══════════════════════════════════════════════════════════

🎯 CHECKLIST FINAL EXPANDIDO

ANTES DA PROVA, CONFIRME QUE SABE:

Section 1 (15%):
□ OPTIMIZE, Z-ORDER (até 4 colunas!), VACUUM, AUTO OPTIMIZE
□ Tipos de compute e quando usar cada um
□ Control Plane vs Data Plane
□ Photon Engine, AQE, Dynamic Partition Pruning

Section 2 (20%):
□ LakeFlow Connect (arquitetura para SaaS, Databases, Storage)
□ Partner Connect (quando usar, exemplos)
□ JSON (STRING vs STRUCT vs VARIANT)
□ Auto Loader vs COPY INTO (diferenças, quando usar)
□ CDC vs SCD (relação, diferenças)
□ Schema Evolution
□ Databricks Connect

Section 3 (30% - A MAIS PESADA!):
□ Medallion Architecture (Bronze/Silver/Gold)
□ DLT / Lakeflow Declarative Pipelines
□ SQL DDL/DML (CREATE, MERGE, UPDATE, DELETE)
□ SCD Type 2 (MERGE manual vs APPLY CHANGES INTO)
□ PySpark (groupBy, agg, count_distinct, window functions)
□ Complex types (ARRAY, MAP, STRUCT, explode, collect_list)

Section 4 (20%):
□ LakeFlow Jobs (estrutura: job → tasks)
□ Control Flow (Sequential, Parallel, If/Else, For Each, Run Job)
□ Triggers (Manual, Scheduled, File Arrival, Table, API, Continuous)
□ Compute types (Interactive, Job, Serverless, SQL Warehouse)
□ Serverless Performance Mode (ON vs OFF)
□ DAB (Databricks Asset Bundles)
□ Repair vs Rerun (Repair = só falhas, Rerun = tudo)
□ Task Values (set/get para passar dados entre tasks)

Section 5 (15%):
□ Managed vs External Tables
□ Unity Catalog (three-level namespace: catalog.schema.table)
□ Permissões (USAGE sempre primeiro!, SELECT, MODIFY, CREATE)
□ Roles (Metastore Admin, Catalog Owner, Schema Owner)
□ Audit Logs (system.access.audit)
□ System Tables (billing.usage, compute.clusters, query.history)
□ Information Schema (tables, columns)
□ Data Lineage
□ Dynamic Views (Row/Column-level security)
□ Service Principals (automação, não usuários)
□ Delta Sharing (vantagens, limitações, custos cross-cloud)
□ Lakehouse Federation (conectar DBs externos)

═══════════════════════════════════════════════════════════

🚀 ESTRATÉGIA DE ESTUDO - ÚLTIMOS DIAS

DIA 1-2: Section 3 (30%)
• Medallion Architecture
• DLT/Lakeflow Declarative Pipelines
• SQL MERGE e SCD Type 2
• PySpark aggregations

DIA 3: Sections 2 e 4 (20% cada)
• LakeFlow Connect + Partner Connect
• Auto Loader vs COPY INTO
• LakeFlow Jobs + Control Flow + Triggers
• Compute types

DIA 4: Sections 1 e 5 (15% cada)
• OPTIMIZE, Z-ORDER, VACUUM
• Unity Catalog permissions
• Delta Sharing

DIA 5: REVISÃO GERAL
• Todas as 15 questões práticas deste guia
• Revisar pontos fracos
• Simulados se disponíveis

═══════════════════════════════════════════════════════════

💪 MENSAGEM FINAL

Você tem agora o guia MAIS COMPLETO disponível para a certificação Databricks Associate Data Engineer!

Este material consolida:
✅ Seu conteúdo original (github.io)
✅ Todo o material dos nossos deep dives
✅ Novo conteúdo sobre LakeFlow Jobs e JSON
✅ 15 questões práticas com explicações
✅ 100% de cobertura da prova

Com dedicação e estudo focado, você está PRONTO para passar! 🎓

Lembre-se:
• 45 questões
• 90 minutos
• 70% para passar (≈32 questões corretas)

BOA SORTE! 🚀

═══════════════════════════════════════════════════════════

👨💻 Créditos:
Material original: Esdras Rocha
Deep dives e expansões: Claude (Anthropic)
Versão final: Outubro 2025

Este é seu guia definitivo. Estude, pratique e conquiste sua certificação! 💯
