<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üéì Databricks Certified Data Engineer Associate - Guia Completo Expandido</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #2d3748;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 40px;
        }
        
        nav {
            background: #f7fafc;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 30px;
            position: sticky;
            top: 20px;
            z-index: 100;
        }
        
        nav h3 {
            color: #4a5568;
            margin-bottom: 15px;
        }
        
        nav ul {
            list-style: none;
        }
        
        nav li {
            margin: 8px 0;
        }
        
        nav a {
            color: #4299e1;
            text-decoration: none;
            transition: color 0.3s;
        }
        
        nav a:hover {
            color: #2c5282;
        }
        
        .section {
            margin: 40px 0;
            padding: 30px;
            background: #f7fafc;
            border-radius: 12px;
            border-left: 5px solid #4299e1;
        }
        
        .section h2 {
            color: #2d3748;
            font-size: 2em;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 15px;
        }
        
        .section h3 {
            color: #2c5282;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
        }
        
        .section h4 {
            color: #4a5568;
            font-size: 1.2em;
            margin: 20px 0 10px 0;
        }
        
        .warning {
            background: #fef5e7;
            border-left: 4px solid #f39c12;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .warning strong {
            color: #c05621;
            font-size: 18px;
        }
        
        .danger {
            background: #fff5f5;
            border-left: 4px solid #f56565;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .danger strong {
            color: #c53030;
            font-size: 18px;
        }
        
        .success {
            background: #f0fff4;
            border-left: 4px solid #48bb78;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .success strong {
            color: #2f855a;
            font-size: 18px;
        }
        
        .exam-tip {
            background: #ebf8ff;
            border-left: 4px solid #3182ce;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .exam-tip strong {
            color: #2c5282;
            font-size: 18px;
        }
        
        pre {
            background: #1a202c;
            color: #68d391;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 14px;
            line-height: 1.6;
            margin: 15px 0;
        }
        
        code {
            background: #edf2f7;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        th {
            background: #4299e1;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #e2e8f0;
        }
        
        tr:hover {
            background: #f7fafc;
        }
        
        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .card {
            background: white;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .highlight {
            background: #fef5e7;
            padding: 3px 8px;
            border-radius: 4px;
            font-weight: 600;
            color: #c05621;
        }
        
        .new-content {
            background: linear-gradient(135deg, #c6f6d5 0%, #9ae6b4 100%);
            border-left: 5px solid #38a169;
            padding: 15px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .new-content strong {
            color: #2f855a;
        }
        
        footer {
            background: #2d3748;
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        footer h3 {
            margin-bottom: 10px;
        }
        
        footer p {
            margin: 5px 0;
            opacity: 0.9;
        }
        
        .progress-bar {
            background: #edf2f7;
            height: 10px;
            border-radius: 5px;
            margin: 10px 0;
            overflow: hidden;
        }
        
        .progress-fill {
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            height: 100%;
            border-radius: 5px;
            transition: width 0.3s;
        }
        
        @media (max-width: 768px) {
            .comparison-grid {
                grid-template-columns: 1fr;
            }
            
            header h1 {
                font-size: 1.8em;
            }
            
            .content {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üéì Databricks Certified Data Engineer Associate</h1>
            <p>Guia Completo Expandido com Detalhes Essenciais para a Prova</p>
            <p style="font-size: 0.9em; margin-top: 10px;">45 quest√µes | 90 minutos | 70% para passar (~32 quest√µes corretas)</p>
        </header>

        <div class="content">
            <nav>
                <h3>üìö Navega√ß√£o R√°pida</h3>
                <ul>
                    <li><a href="#section1">Section 1: Databricks Lakehouse Platform (15%)</a></li>
                    <li><a href="#section2">Section 2: Development and Ingestion (20%)</a></li>
                    <li><a href="#ingestion-deep-dive">üÜï Deep Dive: CDC vs Auto Loader vs COPY INTO vs CTAS</a></li>
                    <li><a href="#section3">Section 3: Data Processing & Transformations (30%)</a></li>
                    <li><a href="#section4">Section 4: Productionizing Data Pipelines (20%)</a></li>
                    <li><a href="#optimization-deep-dive">üÜï Deep Dive: OPTIMIZE, Z-ORDER, VACUUM, AUTO OPTIMIZE</a></li>
                    <li><a href="#section5">Section 5: Data Governance & Quality (15%)</a></li>
                    <li><a href="#exam-questions">üéØ Quest√µes de Prova Pr√°ticas</a></li>
                </ul>
            </nav>

            <!-- NOVO CONTE√öDO: Introdu√ß√£o Expandida -->
            <div class="new-content">
                <strong>üÜï NOVIDADES NESTE GUIA EXPANDIDO:</strong><br>
                ‚úÖ Explica√ß√µes detalhadas de CDC, SCD, Auto Loader, COPY INTO e CTAS com diagramas<br>
                ‚úÖ Compara√ß√µes pr√°ticas: quando usar cada ferramenta de ingest√£o<br>
                ‚úÖ Deep dive completo em OPTIMIZE, Z-ORDER, VACUUM e AUTO OPTIMIZE<br>
                ‚úÖ Cen√°rios reais da ind√∫stria e como resolver<br>
                ‚úÖ 10+ quest√µes pr√°ticas de prova com explica√ß√µes detalhadas<br>
                ‚úÖ Guia visual de arquitetura e fluxos de dados
            </div>

            <!-- Section 1: Original Content -->
            <div id="section1" class="section">
                <h2>üìç Section 1: Databricks Lakehouse Platform (15% - ~7 quest√µes)</h2>
                
                <h3>üéØ Objetivos desta se√ß√£o:</h3>
                <ul style="line-height: 2; margin: 15px 0 15px 30px;">
                    <li>Enable features that simplify data layout decisions and optimize query performance</li>
                    <li>Explain the value of the Data Intelligence Platform</li>
                    <li>Identify the applicable compute to use for a specific use case</li>
                </ul>

                <h3>üèóÔ∏è Arquitetura Databricks Lakehouse Platform</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Camada</th>
                            <th>Descri√ß√£o</th>
                            <th>Localiza√ß√£o</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Control Plane</strong></td>
                            <td>Gerencia notebooks, jobs, UI, metadados</td>
                            <td>Cloud Databricks</td>
                        </tr>
                        <tr>
                            <td><strong>Data Plane</strong></td>
                            <td>Executa processamento, armazena dados</td>
                            <td>Sua conta cloud (AWS/Azure/GCP)</td>
                        </tr>
                        <tr>
                            <td><strong>Storage Layer</strong></td>
                            <td>S3, ADLS, GCS com Delta Lake</td>
                            <td>Sua conta cloud</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning">
                    <strong>‚ö†Ô∏è PARA PROVA - Arquitetura:</strong><br>
                    <ul style="margin-top: 10px; line-height: 2;">
                        <li><strong>Control Plane:</strong> Gerenciado pela Databricks (Workspace, Notebooks, Jobs, Unity Catalog)</li>
                        <li><strong>Data Plane:</strong> Roda na SUA conta cloud (Compute + Storage)</li>
                        <li><strong>Secure Tunnel:</strong> Comunica√ß√£o segura entre Control e Data Plane</li>
                        <li><strong>Dados NUNCA saem do seu cloud:</strong> Databricks n√£o tem acesso aos dados</li>
                        <li><strong>Driver:</strong> Coordena workers e SparkContext</li>
                        <li><strong>Workers:</strong> Executam tasks em paralelo</li>
                    </ul>
                </div>

                <h3>1.3 Tipos de Compute</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Tipo</th>
                            <th>Quando Usar</th>
                            <th>Custo</th>
                            <th>‚ö†Ô∏è Prova</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>All-Purpose Cluster</strong></td>
                            <td>Desenvolvimento interativo, notebooks</td>
                            <td>üí∞üí∞üí∞</td>
                            <td>‚ùå N√ÉO use em produ√ß√£o</td>
                        </tr>
                        <tr>
                            <td><strong>Job Cluster</strong></td>
                            <td>Jobs automatizados, produ√ß√£o</td>
                            <td>üí∞üí∞</td>
                            <td>‚úÖ SEMPRE em produ√ß√£o</td>
                        </tr>
                        <tr>
                            <td><strong>SQL Warehouse</strong></td>
                            <td>Queries SQL, BI tools, dashboards</td>
                            <td>üí∞üí∞</td>
                            <td>Otimizado para SQL</td>
                        </tr>
                        <tr>
                            <td><strong>Serverless</strong></td>
                            <td>Zero gerenciamento, autoscaling</td>
                            <td>üí∞üí∞</td>
                            <td>Hands-off compute</td>
                        </tr>
                    </tbody>
                </table>

                <div class="exam-tip">
                    <strong>‚úÖ DICA PARA PROVA:</strong><br>
                    <ul style="margin-top: 10px; line-height: 2;">
                        <li>Job est√° rodando diariamente? ‚Üí <strong>Job Cluster</strong> (mais barato)</li>
                        <li>Desenvolvimento/explora√ß√£o? ‚Üí <strong>All-Purpose Cluster</strong></li>
                        <li>Dashboard de BI? ‚Üí <strong>SQL Warehouse</strong></li>
                        <li>Quer zero configura√ß√£o? ‚Üí <strong>Serverless</strong></li>
                        <li>M√∫ltiplos usu√°rios? ‚Üí <strong>High Concurrency Mode</strong></li>
                        <li>Performance SQL cr√≠tica? ‚Üí <strong>Photon Engine</strong></li>
                    </ul>
                </div>
            </div>

            <!-- NOVO CONTE√öDO EXPANDIDO: Section 2 com Deep Dive -->
            <div id="section2" class="section">
                <h2>üì• Section 2: Development and Ingestion (20% - ~9 quest√µes)</h2>
                
                <h3>üåä Lakeflow: A Nova Stack de Data Engineering</h3>
                
                <div class="success">
                    <strong>‚≠ê IMPORTANTE - Nomenclatura Atualizada:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8;">
                        A Databricks consolidou toda a stack de Data Engineering sob o guarda-chuva <strong>Lakeflow</strong>. Esta √© a nomenclatura oficial atual e pode aparecer no exame.<br><br>
                        O antigo "Delta Live Tables (DLT)" agora √© chamado de <strong>Lakeflow Declarative Pipelines</strong>.
                    </p>
                </div>

                <table>
                    <thead>
                        <tr>
                            <th>Componente</th>
                            <th>Descri√ß√£o</th>
                            <th>Anteriormente</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>üîå Lakeflow Connect</strong></td>
                            <td>Ingest√£o simplificada de dados com conectores gerenciados</td>
                            <td>Parte do DLT</td>
                        </tr>
                        <tr>
                            <td><strong>‚öôÔ∏è Lakeflow Declarative Pipelines</strong></td>
                            <td>Framework declarativo para pipelines batch e streaming</td>
                            <td>Delta Live Tables (DLT)</td>
                        </tr>
                        <tr>
                            <td><strong>üîÑ Lakeflow Jobs</strong></td>
                            <td>Orquestra√ß√£o confi√°vel e monitoramento de workloads</td>
                            <td>Databricks Workflows</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- NOVO: Deep Dive Completo de Ingest√£o -->
            <div id="ingestion-deep-dive" class="section" style="border-left-color: #48bb78;">
                <h2>üÜï Deep Dive: CDC, SCD, Auto Loader, COPY INTO e CTAS</h2>
                
                <div class="new-content">
                    <strong>üìö CONTE√öDO NOVO E EXPANDIDO:</strong> Esta se√ß√£o detalha completamente as diferen√ßas, rela√ß√µes e n√£o-rela√ß√µes entre CDC, SCD, Auto Loader, COPY INTO e CTAS - conceitos frequentemente confundidos na prova!
                </div>

                <h3>1Ô∏è‚É£ CDC (Change Data Capture)</h3>
                
                <p><strong>O QUE √â:</strong> T√©cnica para capturar <strong>apenas as mudan√ßas</strong> (INSERT, UPDATE, DELETE) de um banco de dados origem.</p>
                
                <p><strong>COMO FUNCIONA:</strong></p>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li>L√™ transaction logs do banco (MySQL binlog, SQL Server CDC, Oracle LogMiner)</li>
                    <li>Captura opera√ß√µes em tempo real ou quase real</li>
                    <li>Traz apenas o "delta" (mudan√ßas), n√£o tudo de novo</li>
                </ul>

                <p><strong>QUANDO USAR:</strong> Dados transacionais com muitas altera√ß√µes (bancos OLTP)</p>

                <h4>üìç Como ler CDC no Databricks:</h4>
                
                <table>
                    <thead>
                        <tr>
                            <th>M√©todo</th>
                            <th>Como Funciona</th>
                            <th>Quando Usar</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Debezium + Kafka</strong></td>
                            <td>Debezium captura CDC ‚Üí Kafka ‚Üí Databricks Structured Streaming</td>
                            <td>Mais comum em produ√ß√£o</td>
                        </tr>
                        <tr>
                            <td><strong>JDBC direto</strong></td>
                            <td>L√™ tabelas CDC nativas do banco via JDBC</td>
                            <td>Conex√£o direta, sem middleware</td>
                        </tr>
                        <tr>
                            <td><strong>Lakeflow Connect</strong></td>
                            <td>Ferramenta nativa do Databricks com UI</td>
                            <td>Mais f√°cil, gerenciado</td>
                        </tr>
                    </tbody>
                </table>

                <div class="danger">
                    <strong>üö® CR√çTICO PARA A PROVA:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8;">
                        <strong>CDC S√ì FUNCIONA EM BANCOS DE DADOS!</strong><br>
                        CDC precisa do transaction log (binlog, WAL, redo log) que s√≥ bancos t√™m.<br>
                        Arquivos (CSV, JSON, Parquet) <strong>N√ÉO T√äM transaction log</strong>, portanto n√£o podem usar CDC tradicional.
                    </p>
                </div>

                <h3>2Ô∏è‚É£ SCD (Slowly Changing Dimensions)</h3>
                
                <p><strong>O QUE √â:</strong> T√©cnica de <strong>modelagem dimensional</strong> para rastrear mudan√ßas hist√≥ricas em dados.</p>
                
                <table>
                    <thead>
                        <tr>
                            <th>Tipo</th>
                            <th>O que faz</th>
                            <th>Quando usar</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>SCD Type 1</strong></td>
                            <td>Sobrescreve (perde hist√≥rico)</td>
                            <td>Corre√ß√µes, dados n√£o hist√≥ricos</td>
                        </tr>
                        <tr>
                            <td><strong>SCD Type 2</strong></td>
                            <td>Cria nova linha com versionamento (mant√©m hist√≥rico)</td>
                            <td>Auditoria, an√°lise temporal</td>
                        </tr>
                        <tr>
                            <td><strong>SCD Type 3</strong></td>
                            <td>Adiciona coluna para valor anterior</td>
                            <td>Mudan√ßas ocasionais</td>
                        </tr>
                    </tbody>
                </table>

                <div class="success">
                    <strong>üîó RELA√á√ÉO CDC ‚ÜîÔ∏è SCD:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8;">
                        ‚Ä¢ <strong>CDC captura as mudan√ßas</strong> da origem (o QUE mudou)<br>
                        ‚Ä¢ <strong>SCD define COMO voc√™ armazena</strong> essas mudan√ßas no destino (COM ou SEM hist√≥rico)<br>
                        ‚Ä¢ Voc√™ usa CDC para <strong>detectar</strong> mudan√ßas e SCD para <strong>modelar</strong> o hist√≥rico
                    </p>
                </div>

                <h3>3Ô∏è‚É£ Auto Loader</h3>
                
                <p><strong>O QUE √â:</strong> Ferramenta do Databricks para <strong>ingest√£o incremental de arquivos</strong> (CSV, JSON, Parquet, etc) de cloud storage.</p>
                
                <p><strong>COMO FUNCIONA:</strong></p>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li>Monitora um diret√≥rio (S3, ADLS, GCS)</li>
                    <li>Detecta automaticamente novos arquivos</li>
                    <li>Processa apenas arquivos novos (incremental)</li>
                    <li>Schema inference e evolution autom√°ticos</li>
                </ul>

                <pre># Auto Loader - ingere novos arquivos automaticamente
df = (spark.readStream
    .format("cloudFiles")  # ‚Üê Auto Loader
    .option("cloudFiles.format", "json")
    .option("cloudFiles.schemaLocation", "/path/schema")
    .load("/mnt/raw-data/")
)

df.writeStream.table("bronze.eventos")</pre>

                <h3>4Ô∏è‚É£ COPY INTO</h3>
                
                <p><strong>O QUE √â:</strong> Comando SQL do Databricks para <strong>carregar dados de arquivos para tabelas Delta</strong> de forma idempotente.</p>
                
                <p><strong>COMO FUNCIONA:</strong></p>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li>Carrega dados de arquivos (CSV, JSON, Parquet)</li>
                    <li><strong>Rastreia quais arquivos j√° foram processados</strong> (evita duplicatas)</li>
                    <li>Execu√ß√£o mais simples que Auto Loader (batch, n√£o streaming)</li>
                </ul>

                <pre>-- COPY INTO - carrega apenas arquivos novos
COPY INTO bronze.vendas
FROM '/mnt/raw-data/vendas/'
FILEFORMAT = CSV
FORMAT_OPTIONS ('header' = 'true')
COPY_OPTIONS ('mergeSchema' = 'true')</pre>

                <h3>üìä Auto Loader vs COPY INTO - Compara√ß√£o Completa</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Caracter√≠stica</th>
                            <th>Auto Loader</th>
                            <th>COPY INTO</th>
                            <th>Vencedor</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Tipo de Processamento</strong></td>
                            <td>Streaming (cont√≠nuo)</td>
                            <td>Batch (execu√ß√£o pontual)</td>
                            <td>üèÜ Auto Loader (mais moderno)</td>
                        </tr>
                        <tr>
                            <td><strong>Schema Evolution</strong></td>
                            <td>‚úÖ Autom√°tico</td>
                            <td>‚ùå Manual</td>
                            <td>üèÜ Auto Loader</td>
                        </tr>
                        <tr>
                            <td><strong>Performance com milh√µes de arquivos</strong></td>
                            <td>‚úÖ Excelente (notifica√ß√£o de eventos)</td>
                            <td>‚ùå Degrada (precisa listar todos)</td>
                            <td>üèÜ Auto Loader</td>
                        </tr>
                        <tr>
                            <td><strong>Simplicidade (c√≥digo)</strong></td>
                            <td>Mais verboso (precisa configurar)</td>
                            <td>‚úÖ SQL simples (1-2 linhas)</td>
                            <td>üèÜ COPY INTO</td>
                        </tr>
                        <tr>
                            <td><strong>Recomenda√ß√£o Databricks</strong></td>
                            <td>‚úÖ Best practice padr√£o</td>
                            <td>‚ö†Ô∏è Casos espec√≠ficos</td>
                            <td>üèÜ Auto Loader</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning">
                    <strong>‚ö†Ô∏è Por que COPY INTO est√° virando Legacy:</strong><br>
                    <ul style="margin-top: 10px; line-height: 2;">
                        <li>Limita√ß√µes t√©cnicas intranspon√≠veis (batch only, lista todos arquivos)</li>
                        <li>Auto Loader faz TUDO que COPY INTO faz + muito mais</li>
                        <li>Databricks parou de investir em COPY INTO (sem features novas desde 2020)</li>
                        <li>Tend√™ncia da ind√∫stria: streaming-first</li>
                    </ul>
                </div>

                <div class="exam-tip">
                    <strong>üéØ PARA A PROVA - Quando usar cada um:</strong><br>
                    <ul style="margin-top: 10px; line-height: 2;">
                        <li><strong>Auto Loader:</strong> SEMPRE que poss√≠vel (produ√ß√£o, escala, streaming)</li>
                        <li><strong>COPY INTO:</strong> Apenas para batch simples, poucos arquivos, dev/test</li>
                        <li>Se a pergunta n√£o especificar restri√ß√µes, <strong>Auto Loader √© sempre a resposta</strong></li>
                    </ul>
                </div>

                <h3>5Ô∏è‚É£ CTAS (Create Table As Select)</h3>
                
                <p><strong>O QUE √â:</strong> Comando SQL para <strong>criar uma tabela a partir de uma query</strong>.</p>
                
                <div class="success">
                    <strong>‚úÖ VERDADE sobre CTAS:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8;">
                        CTAS <strong>N√ÉO precisa de Volumes!</strong><br>
                        CTAS trabalha com qualquer tabela j√° no Databricks (Delta Lake, tabelas no Unity Catalog).<br>
                        <strong>CTAS = transforma√ß√£o</strong>, n√£o ingest√£o!
                    </p>
                </div>

                <pre>-- CTAS - cria tabela a partir de query
CREATE TABLE gold.vendas_sumarizadas AS
SELECT 
    data,
    produto,
    SUM(valor) as total_vendas
FROM silver.vendas
GROUP BY data, produto</pre>

                <h3>üéØ Resumo: Quando usar cada um</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Cen√°rio</th>
                            <th>Use</th>
                            <th>Por qu√™</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Banco de dados transacional com mudan√ßas frequentes</td>
                            <td><strong>CDC</strong></td>
                            <td>Captura mudan√ßas do transaction log</td>
                        </tr>
                        <tr>
                            <td>Precisa manter hist√≥rico de mudan√ßas</td>
                            <td><strong>SCD Type 2</strong></td>
                            <td>Rastreia "como era" vs "como est√°"</td>
                        </tr>
                        <tr>
                            <td>Arquivos chegam continuamente no S3/ADLS</td>
                            <td><strong>Auto Loader</strong></td>
                            <td>Streaming, schema evolution, escala</td>
                        </tr>
                        <tr>
                            <td>Carga batch simples de poucos arquivos</td>
                            <td><strong>COPY INTO</strong></td>
                            <td>SQL simples, idempotente</td>
                        </tr>
                        <tr>
                            <td>Criar tabela agregada de dados existentes</td>
                            <td><strong>CTAS</strong></td>
                            <td>Transforma√ß√£o/materializa√ß√£o</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- Section 3: Original + Expanded -->
            <div id="section3" class="section">
                <h2>‚öôÔ∏è Section 3: Data Processing & Transformations (30% - 13-14 quest√µes)</h2>
                
                <h3>3.1 Medallion Architecture (‚≠ê FUNDAMENTAL)</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Layer</th>
                            <th>Prop√≥sito</th>
                            <th>O que fazer</th>
                            <th>O que N√ÉO fazer</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>ü•â Bronze</strong></td>
                            <td>Raw data, source of truth</td>
                            <td>Ingest√£o pura, metadados</td>
                            <td>‚ùå Sem transforma√ß√µes, filtros, limpeza</td>
                        </tr>
                        <tr>
                            <td><strong>ü•à Silver</strong></td>
                            <td>Cleaned, validated</td>
                            <td>Limpar, validar, dedupe, joins</td>
                            <td>‚ùå Sem agrega√ß√µes de neg√≥cio</td>
                        </tr>
                        <tr>
                            <td><strong>ü•á Gold</strong></td>
                            <td>Business-level, curated</td>
                            <td>Agregar, KPIs, m√©tricas</td>
                            <td>Pronto para BI/ML</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning">
                    <strong>‚ö†Ô∏è PERGUNTAS COMUNS NA PROVA:</strong><br>
                    <ul style="margin-top: 10px; line-height: 2;">
                        <li>Onde remover duplicatas? ‚Üí <strong>Silver</strong></li>
                        <li>Onde est√° o dado exatamente como veio da fonte? ‚Üí <strong>Bronze</strong></li>
                        <li>Onde criar tabela agregada para dashboard? ‚Üí <strong>Gold</strong></li>
                        <li>Onde aplicar business rules? ‚Üí <strong>Silver ‚Üí Gold</strong></li>
                    </ul>
                </div>

                <h3>3.4 SQL DDL/DML (‚≠ê MUITAS QUEST√ïES)</h3>
                
                <h4>DDL (Data Definition Language):</h4>
                
                <pre>-- CREATE OR REPLACE: Substitui se existe
CREATE OR REPLACE TABLE users (
    id INT,
    name STRING
) USING DELTA;

-- CTAS (Create Table As Select)
CREATE TABLE active_users AS
SELECT * FROM users
WHERE last_login > current_date() - 30;</pre>

                <h4>DML (Data Manipulation Language):</h4>
                
                <pre>-- MERGE (upsert - MUITO IMPORTANTE!)
MERGE INTO users AS target
USING updates AS source
ON target.id = source.id
WHEN MATCHED THEN
    UPDATE SET target.email = source.email
WHEN NOT MATCHED THEN
    INSERT (id, name, email)
    VALUES (source.id, source.name, source.email);</pre>
            </div>

            <!-- Section 4: Original -->
            <div id="section4" class="section">
                <h2>üöÄ Section 4: Productionizing Data Pipelines (20% - 9 quest√µes)</h2>
                
                <h3>4.2 Lakeflow Jobs (Workflows) - Deploy, Repair, Rerun</h3>
                
                <div class="success">
                    <strong>üìù Nomenclatura Atualizada:</strong> Databricks Workflows agora s√£o chamados de <strong>Lakeflow Jobs</strong> - parte da stack Lakeflow unificada para Data Engineering.
                </div>

                <h4>Conceitos importantes:</h4>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li><strong>Task:</strong> Unidade de trabalho (notebook, Python, SQL, JAR, DLT)</li>
                    <li><strong>DAG:</strong> Directed Acyclic Graph - ordem de execu√ß√£o</li>
                    <li><strong>Dependencies:</strong> Task B depende de Task A</li>
                    <li><strong>Job Cluster:</strong> Cluster criado para o job (recomendado em produ√ß√£o)</li>
                </ul>

                <div class="exam-tip">
                    <strong>üéØ REPAIR vs RERUN:</strong><br>
                    <ul style="margin-top: 10px; line-height: 2;">
                        <li><strong>Repair:</strong> Re-executa APENAS tasks que falharam</li>
                        <li><strong>Rerun:</strong> Re-executa TODAS as tasks</li>
                        <li><strong>Na prova:</strong> Se perguntarem sobre otimiza√ß√£o/economia, sempre escolha Repair</li>
                    </ul>
                </div>
            </div>

            <!-- NOVO: Deep Dive Completo de Otimiza√ß√£o -->
            <div id="optimization-deep-dive" class="section" style="border-left-color: #f39c12;">
                <h2>üÜï Deep Dive: OPTIMIZE, Z-ORDER, VACUUM e AUTO OPTIMIZE</h2>
                
                <div class="new-content">
                    <strong>üìö CONTE√öDO NOVO E EXPANDIDO:</strong> Esta se√ß√£o explica em detalhes como funcionam os comandos de otimiza√ß√£o Delta Lake, seus trade-offs e quando usar cada um. Conte√∫do cr√≠tico para a prova!
                </div>

                <h3>1Ô∏è‚É£ OPTIMIZE - Compacta√ß√£o de Arquivos</h3>
                
                <p><strong>O QUE FAZ:</strong> Compacta arquivos pequenos em arquivos grandes (~1GB) para melhorar performance de leitura.</p>
                
                <p><strong>PROBLEMA QUE RESOLVE:</strong> Delta Lake cria muitos arquivos pequenos ao longo do tempo (cada write, update, delete gera novos arquivos). Muitos arquivos pequenos = performance ruim.</p>

                <pre>-- Otimizar tabela inteira
OPTIMIZE vendas;

-- Otimizar apenas uma parti√ß√£o
OPTIMIZE vendas WHERE ano = 2025 AND mes = 10;</pre>

                <div class="danger">
                    <strong>‚ö†Ô∏è CR√çTICO PARA A PROVA:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8; font-size: 16px;">
                        <strong>OPTIMIZE N√ÉO DELETA ARQUIVOS FISICAMENTE!</strong><br>
                        Ele apenas marca os arquivos antigos como "removidos" no transaction log.<br>
                        Os arquivos f√≠sicos continuam no storage!<br>
                        Para deletar fisicamente, voc√™ precisa rodar <code>VACUUM</code> depois.
                    </p>
                </div>

                <h4>üî¨ Como OPTIMIZE Funciona Internamente:</h4>
                <ol style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li><strong>Identifica√ß√£o:</strong> Identifica arquivos pequenos (&lt; 1 GB padr√£o)</li>
                    <li><strong>Leitura:</strong> L√™ todos os arquivos pequenos identificados</li>
                    <li><strong>Merge:</strong> Combina os dados em arquivos maiores (~1 GB cada)</li>
                    <li><strong>Escrita:</strong> Escreve novos arquivos otimizados</li>
                    <li><strong>Atualiza√ß√£o Transaction Log:</strong> Marca arquivos antigos como "removidos" (logicamente), adiciona novos arquivos ao log</li>
                </ol>

                <h3>2Ô∏è‚É£ Z-ORDER - Organiza√ß√£o por Colunas</h3>
                
                <p><strong>O QUE √â:</strong> T√©cnica de <strong>co-localiza√ß√£o de dados relacionados</strong> dentro dos arquivos Delta usando uma curva Z-order (space-filling curve).</p>
                
                <p><strong>RESULTADO:</strong> Queries com filtros nessas colunas pulam mais dados (data skipping) = mais r√°pidas.</p>

                <pre>-- Z-ORDER sempre √© usado JUNTO com OPTIMIZE
OPTIMIZE vendas
ZORDER BY (produto_id, regiao);

-- Z-ORDER em parti√ß√£o espec√≠fica
OPTIMIZE vendas
WHERE ano = 2025
ZORDER BY (produto_id, cidade);</pre>

                <div class="warning">
                    <strong>‚ö†Ô∏è CR√çTICO PARA A PROVA - Limite de Colunas:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8;">
                        <strong>Z-ORDER funciona bem com AT√â 4 COLUNAS!</strong><br>
                        ‚Ä¢ 1-2 colunas: Excelente<br>
                        ‚Ä¢ 3-4 colunas: Bom, mas retorno diminui<br>
                        ‚Ä¢ 5+ colunas: ‚ùå Performance piora! (curse of dimensionality)<br>
                        <br>
                        <strong>Por qu√™?</strong> Com muitas dimens√µes, os dados ficam espalhados novamente. O benef√≠cio do data skipping diminui drasticamente.
                    </p>
                </div>

                <h4>üéØ Escolhendo Colunas para Z-ORDER:</h4>
                
                <table>
                    <thead>
                        <tr>
                            <th>Crit√©rio</th>
                            <th>Boa Escolha?</th>
                            <th>Explica√ß√£o</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Coluna usada em WHERE frequentemente</td>
                            <td>‚úÖ‚úÖ‚úÖ EXCELENTE</td>
                            <td>M√°ximo benef√≠cio de data skipping</td>
                        </tr>
                        <tr>
                            <td>Coluna com alta cardinalidade</td>
                            <td>‚úÖ‚úÖ BOM</td>
                            <td>Mais valores √∫nicos = melhor distribui√ß√£o</td>
                        </tr>
                        <tr>
                            <td>Coluna de particionamento</td>
                            <td>‚ùå N√ÉO</td>
                            <td>J√° est√° particionado, Z-ORDER redundante</td>
                        </tr>
                        <tr>
                            <td>Coluna raramente usada em queries</td>
                            <td>‚ùå N√ÉO</td>
                            <td>Desperd√≠cio, n√£o traz benef√≠cio</td>
                        </tr>
                    </tbody>
                </table>

                <h3>3Ô∏è‚É£ VACUUM - Limpeza F√≠sica de Arquivos</h3>
                
                <p><strong>O QUE FAZ:</strong> <strong>DELETA FISICAMENTE</strong> arquivos antigos que n√£o s√£o mais referenciados pelo transaction log.</p>
                
                <p><strong>POR QUE PRECISA:</strong> Opera√ß√µes como OPTIMIZE, UPDATE, DELETE, MERGE marcam arquivos como "removidos" logicamente, mas n√£o deletam do storage. VACUUM faz a limpeza f√≠sica.</p>

                <pre>-- VACUUM com retention padr√£o (7 dias)
VACUUM vendas;

-- VACUUM com retention customizado
VACUUM vendas RETAIN 168 HOURS;  -- 7 dias
VACUUM vendas RETAIN 24 HOURS;   -- 1 dia

-- Dry run (ver o que seria deletado sem deletar)
VACUUM vendas DRY RUN;</pre>

                <div class="danger">
                    <strong>üö® CR√çTICO PARA A PROVA - VACUUM vs Time Travel:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8; font-size: 16px;">
                        <strong>VACUUM IMPEDE TIME TRAVEL das vers√µes removidas!</strong><br>
                        <br>
                        Se voc√™ rodar <code>VACUUM RETAIN 7 DAYS</code>, voc√™:<br>
                        ‚úÖ Pode fazer Time Travel dos √∫ltimos 7 dias<br>
                        ‚ùå N√ÉO pode fazer Time Travel de mais de 7 dias atr√°s<br>
                        <br>
                        <strong>Por qu√™?</strong> VACUUM deletou os arquivos f√≠sicos! N√£o tem como restaurar sem os arquivos.
                    </p>
                </div>

                <h4>‚è∞ Retention Period (Per√≠odo de Reten√ß√£o):</h4>
                
                <table>
                    <thead>
                        <tr>
                            <th>Retention</th>
                            <th>Quando Usar?</th>
                            <th>Trade-off</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>7 dias (padr√£o)</strong></td>
                            <td>Produ√ß√£o normal</td>
                            <td>‚úÖ Balanceado: Time Travel razo√°vel + economia de espa√ßo</td>
                        </tr>
                        <tr>
                            <td><strong>30 dias</strong></td>
                            <td>Compliance, auditoria</td>
                            <td>‚úÖ Mais Time Travel<br>‚ùå Mais espa√ßo ocupado</td>
                        </tr>
                        <tr>
                            <td><strong>24 horas</strong></td>
                            <td>Ambiente dev/test</td>
                            <td>‚úÖ Libera espa√ßo r√°pido<br>‚ùå Time Travel limitado</td>
                        </tr>
                        <tr>
                            <td><strong>0 horas (‚ö†Ô∏è)</strong></td>
                            <td>‚ùå Evite! Perigoso!</td>
                            <td>‚ùå Sem Time Travel<br>‚ùå Pode corromper queries em execu√ß√£o</td>
                        </tr>
                    </tbody>
                </table>

                <h3>4Ô∏è‚É£ AUTO OPTIMIZE - Otimiza√ß√£o Autom√°tica</h3>
                
                <p><strong>O QUE √â:</strong> Feature que executa <strong>OPTIMIZE automaticamente durante a escrita</strong>.</p>
                
                <p><strong>COMPOSTO POR 2 FEATURES:</strong></p>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li><strong>Optimized Writes:</strong> escreve arquivos maiores desde o in√≠cio</li>
                    <li><strong>Auto Compaction:</strong> compacta arquivos pequenos automaticamente ap√≥s write</li>
                </ul>

                <pre>-- Habilitar AUTO OPTIMIZE
ALTER TABLE vendas SET TBLPROPERTIES (
    'delta.autoOptimize.optimizeWrite' = 'true',
    'delta.autoOptimize.autoCompact' = 'true'
);</pre>

                <div class="warning">
                    <strong>‚ö†Ô∏è CR√çTICO PARA A PROVA - Trade-off:</strong><br>
                    <p style="margin-top: 10px; line-height: 1.8; font-size: 16px;">
                        <strong>AUTO OPTIMIZE adiciona PEQUENO overhead na escrita, mas GRANDE ganho na leitura!</strong><br>
                        <br>
                        <strong>Write performance:</strong> ‚¨áÔ∏è ~5-10% mais lento (reorganiza√ß√£o)<br>
                        <strong>Read performance:</strong> ‚¨ÜÔ∏è 3-10x mais r√°pido (arquivos otimizados)<br>
                        <br>
                        <strong>Quando vale a pena?</strong><br>
                        ‚úÖ Tabelas read-heavy (muito mais leitura que escrita)<br>
                        ‚úÖ Streaming cont√≠nuo (muitos arquivos pequenos)<br>
                        ‚úÖ Workloads anal√≠ticos<br>
                        <br>
                        <strong>Quando N√ÉO vale?</strong><br>
                        ‚ùå Tabelas write-heavy com poucas leituras<br>
                        ‚ùå ETL batch r√°pido onde write time √© cr√≠tico
                    </p>
                </div>

                <h3>üìä Matriz de Compara√ß√£o: Todos os Comandos</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Comando</th>
                            <th>O que faz</th>
                            <th>Deleta f√≠sico?</th>
                            <th>Impacta Time Travel?</th>
                            <th>Quando usar</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>OPTIMIZE</strong></td>
                            <td>Compacta arquivos pequenos</td>
                            <td>‚ùå N√£o</td>
                            <td>‚ùå N√£o</td>
                            <td>Muitos writes, updates, deletes</td>
                        </tr>
                        <tr>
                            <td><strong>Z-ORDER</strong></td>
                            <td>Co-localiza dados por colunas</td>
                            <td>‚ùå N√£o</td>
                            <td>‚ùå N√£o</td>
                            <td>Queries com filtros espec√≠ficos</td>
                        </tr>
                        <tr>
                            <td><strong>VACUUM</strong></td>
                            <td>Deleta arquivos antigos</td>
                            <td>‚úÖ Sim!</td>
                            <td>‚úÖ Sim! (remove vers√µes antigas)</td>
                            <td>Ap√≥s OPTIMIZE, liberar espa√ßo</td>
                        </tr>
                        <tr>
                            <td><strong>AUTO OPTIMIZE</strong></td>
                            <td>OPTIMIZE autom√°tico</td>
                            <td>‚ùå N√£o</td>
                            <td>‚ùå N√£o</td>
                            <td>Streaming, zero-maintenance</td>
                        </tr>
                    </tbody>
                </table>

                <h3>üéØ Workflow Recomendado de Otimiza√ß√£o</h3>
                
                <ol style="margin: 15px 0 15px 30px; line-height: 2.5;">
                    <li><strong>Etapa 1: Habilitar AUTO OPTIMIZE (se apropriado)</strong> - Para tabelas streaming ou com writes frequentes
                        <pre>ALTER TABLE vendas SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
);</pre>
                    </li>
                    <li><strong>Etapa 2: OPTIMIZE manual periodicamente</strong> - Semanal ou ap√≥s grandes cargas
                        <pre>OPTIMIZE vendas
ZORDER BY (produto_id, regiao, data);</pre>
                    </li>
                    <li><strong>Etapa 3: VACUUM para liberar espa√ßo</strong> - Ap√≥s OPTIMIZE, respeitando retention
                        <pre>VACUUM vendas RETAIN 168 HOURS;</pre>
                    </li>
                    <li><strong>Etapa 4: Monitorar e ajustar</strong> - Verificar m√©tricas e ajustar frequ√™ncia
                        <pre>DESCRIBE DETAIL vendas;
DESCRIBE HISTORY vendas;</pre>
                    </li>
                </ol>
            </div>

            <!-- Section 5: Original -->
            <div id="section5" class="section">
                <h2>üîí Section 5: Data Governance & Quality (15% - 7 quest√µes)</h2>
                
                <h3>5.1 Managed vs External Tables</h3>
                
                <table>
                    <thead>
                        <tr>
                            <th>Aspecto</th>
                            <th>Managed Table</th>
                            <th>External Table</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Gerenciamento</strong></td>
                            <td>UC gerencia dados + metadados</td>
                            <td>UC gerencia s√≥ metadados</td>
                        </tr>
                        <tr>
                            <td><strong>Localiza√ß√£o</strong></td>
                            <td>Autom√°tica (UC define)</td>
                            <td>Voc√™ especifica (LOCATION)</td>
                        </tr>
                        <tr>
                            <td><strong>DROP TABLE</strong></td>
                            <td>‚ùå Deleta dados + metadados</td>
                            <td>‚úÖ Deleta s√≥ metadados</td>
                        </tr>
                        <tr>
                            <td><strong>Quando usar</strong></td>
                            <td>Dados internos do lakehouse</td>
                            <td>Dados compartilhados/externos</td>
                        </tr>
                    </tbody>
                </table>

                <h3>5.2 Unity Catalog - Permiss√µes</h3>
                
                <h4>Three-Level Namespace (‚≠ê‚≠ê MUITO IMPORTANTE!):</h4>
                <p>Estrutura: <code>catalog.schema.table</code></p>

                <table>
                    <thead>
                        <tr>
                            <th>Permiss√£o</th>
                            <th>O que permite</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>USAGE</strong></td>
                            <td>Necess√°rio antes de qualquer outro (catalog/schema)</td>
                        </tr>
                        <tr>
                            <td><strong>SELECT</strong></td>
                            <td>Ler dados (queries)</td>
                        </tr>
                        <tr>
                            <td><strong>MODIFY</strong></td>
                            <td>INSERT, UPDATE, DELETE, MERGE</td>
                        </tr>
                        <tr>
                            <td><strong>CREATE</strong></td>
                            <td>Criar objetos (tables, schemas)</td>
                        </tr>
                    </tbody>
                </table>

                <pre>-- USAGE √© sempre necess√°rio primeiro!
GRANT USAGE ON CATALOG prod TO analysts;
GRANT USAGE ON SCHEMA prod.sales TO analysts;
GRANT SELECT ON TABLE prod.sales.customers TO analysts;</pre>

                <h3>5.8 Delta Sharing (‚≠ê IMPORTANTE)</h3>
                
                <p><strong>O QUE √â:</strong> Protocolo aberto para compartilhar dados entre organiza√ß√µes/plataformas</p>
                
                <h4>Vantagens:</h4>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li>‚úÖ Sem c√≥pia de dados (economia de storage)</li>
                    <li>‚úÖ Sempre atualizado (live data)</li>
                    <li>‚úÖ Governan√ßa centralizada via UC</li>
                    <li>‚úÖ Audit logs autom√°ticos</li>
                    <li>‚úÖ Revoga√ß√£o instant√¢nea</li>
                    <li>‚úÖ Cross-cloud (AWS, Azure, GCP)</li>
                </ul>

                <h4>Limita√ß√µes:</h4>
                <ul style="margin: 15px 0 15px 30px; line-height: 2;">
                    <li>‚ùå READ-only (exceto D2D)</li>
                    <li>‚ùå Apenas Delta Tables</li>
                    <li>‚ùå Sem streaming</li>
                    <li>‚ùå Custos de egress (cross-cloud)</li>
                </ul>
            </div>

            <!-- NOVO: Quest√µes de Prova -->
            <div id="exam-questions" class="section" style="border-left-color: #9f7aea;">
                <h2>üéØ Quest√µes de Prova Pr√°ticas</h2>
                
                <div class="new-content">
                    <strong>üìù CONTE√öDO NOVO:</strong> 10 quest√µes pr√°ticas estilo exame com respostas detalhadas e explica√ß√µes do racioc√≠nio!
                </div>

                <h3>Quest√£o 1: Ingest√£o de Dados</h3>
                <p><strong>Voc√™ precisa ingerir dados de um banco PostgreSQL transacional que recebe milhares de updates por minuto. Qual abordagem √© mais eficiente?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>Full refresh a cada 5 minutos</li>
                    <li>Incremental load baseado em timestamp</li>
                    <li>Change Data Capture (CDC)</li>
                    <li>Export manual di√°rio</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: C) Change Data Capture (CDC)</strong><br>
                    <p style="margin-top: 10px;">CDC captura apenas as mudan√ßas (inserts, updates, deletes) em tempo real ou quase real, sem sobrecarregar o banco de origem. Full refresh carregaria todos os dados repetidamente (ineficiente). Incremental com timestamp pode perder updates em registros antigos.</p>
                </div>

                <h3>Quest√£o 2: Seguran√ßa</h3>
                <p><strong>Ao configurar uma conex√£o para ingest√£o de dados sens√≠veis de um banco SQL Server, onde voc√™ DEVE armazenar as credenciais?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>Hardcoded no notebook</li>
                    <li>Em vari√°veis de ambiente do cluster</li>
                    <li>No Databricks Secrets (Secret Scope)</li>
                    <li>Em um arquivo de configura√ß√£o no DBFS</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: C) No Databricks Secrets (Secret Scope)</strong><br>
                    <p style="margin-top: 10px;">Databricks Secrets √© a forma segura e recomendada de armazenar credenciais, tokens e passwords. As outras op√ß√µes exp√µem informa√ß√µes sens√≠veis.</p>
                </div>

                <h3>Quest√£o 3: Formato de Dados</h3>
                <p><strong>Voc√™ est√° ingerindo dados de um bucket S3 que recebe novos arquivos CSV a cada hora. Qual formato de tabela voc√™ deve usar como destino para melhor performance e recursos ACID?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>Parquet</li>
                    <li>CSV</li>
                    <li>Delta Lake</li>
                    <li>JSON</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: C) Delta Lake</strong><br>
                    <p style="margin-top: 10px;">Delta Lake oferece: ACID transactions, Time travel, Schema enforcement, Melhor performance que Parquet para leitura/escrita. Na prova: qualquer pergunta sobre "formato recomendado no Databricks" ‚Üí Delta Lake √© quase sempre a resposta.</p>
                </div>

                <h3>Quest√£o 4: Auto Loader vs COPY INTO</h3>
                <p><strong>Arquivos JSON chegam a cada 5 minutos no S3 e voc√™ precisa process√°-los com baixa lat√™ncia. Qual solu√ß√£o usar?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>COPY INTO agendado a cada 5 minutos</li>
                    <li>Auto Loader com streaming</li>
                    <li>Spark batch job</li>
                    <li>Manual download e upload</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: B) Auto Loader com streaming</strong><br>
                    <p style="margin-top: 10px;">Auto Loader streaming processa arquivos assim que chegam com baixa lat√™ncia. COPY INTO √© batch e menos eficiente para processamento cont√≠nuo. Schema evolution autom√°tico √© bonus.</p>
                </div>

                <h3>Quest√£o 5: OPTIMIZE e VACUUM</h3>
                <p><strong>Ap√≥s executar OPTIMIZE em uma tabela, voc√™ nota que o espa√ßo em disco n√£o diminuiu. Por qu√™?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>OPTIMIZE n√£o compacta arquivos corretamente</li>
                    <li>OPTIMIZE n√£o deleta arquivos f√≠sicos, apenas marca como removidos</li>
                    <li>Precisa rodar OPTIMIZE duas vezes</li>
                    <li>Erro na execu√ß√£o do comando</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: B) OPTIMIZE n√£o deleta arquivos f√≠sicos, apenas marca como removidos</strong><br>
                    <p style="margin-top: 10px;">OPTIMIZE n√£o deleta arquivos f√≠sicos, apenas marca como removidos no transaction log. Para liberar espa√ßo, execute VACUUM depois.</p>
                </div>

                <h3>Quest√£o 6: Z-ORDER</h3>
                <p><strong>Voc√™ tem uma tabela com queries frequentes por produto_id, cidade, categoria, vendedor_id e regi√£o. Como otimizar?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>ZORDER BY (produto_id, cidade, categoria, vendedor_id, regi√£o)</li>
                    <li>ZORDER BY (produto_id)</li>
                    <li>ZORDER BY (produto_id, cidade, categoria)</li>
                    <li>N√£o usar Z-ORDER</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: C) ZORDER BY (produto_id, cidade, categoria)</strong><br>
                    <p style="margin-top: 10px;">Z-ORDER funciona bem com at√© 4 colunas. 5+ colunas degrada performance (curse of dimensionality). Escolha as 3-4 colunas MAIS USADAS em filtros.</p>
                </div>

                <h3>Quest√£o 7: VACUUM e Time Travel</h3>
                <p><strong>Voc√™ executou VACUUM RETAIN 0 HOURS e agora n√£o consegue fazer Time Travel de ontem. O que aconteceu?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>Bug no Databricks</li>
                    <li>VACUUM deletou os arquivos f√≠sicos necess√°rios para Time Travel</li>
                    <li>Precisa recriar a tabela</li>
                    <li>Time Travel n√£o funciona com VACUUM</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: B) VACUUM deletou os arquivos f√≠sicos necess√°rios para Time Travel</strong><br>
                    <p style="margin-top: 10px;">VACUUM deletou os arquivos f√≠sicos necess√°rios. Com RETAIN 0 HOURS, todos os arquivos antigos foram removidos imediatamente, impossibilitando Time Travel.</p>
                </div>

                <h3>Quest√£o 8: AUTO OPTIMIZE</h3>
                <p><strong>Voc√™ habilitou AUTO OPTIMIZE em uma tabela de streaming e notou que o write latency aumentou 10%. Isso √© esperado?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>N√£o, √© um bug</li>
                    <li>Sim, AUTO OPTIMIZE adiciona overhead na escrita mas melhora leitura</li>
                    <li>N√£o, deve desabilitar imediatamente</li>
                    <li>Sim, mas n√£o h√° benef√≠cio</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: B) Sim, AUTO OPTIMIZE adiciona overhead na escrita mas melhora leitura</strong><br>
                    <p style="margin-top: 10px;">Sim, √© esperado. AUTO OPTIMIZE adiciona pequeno overhead na escrita (~5-10%) porque reorganiza dados para criar arquivos otimizados. O benef√≠cio √© na leitura, que fica 3-10x mais r√°pida.</p>
                </div>

                <h3>Quest√£o 9: Medallion Architecture</h3>
                <p><strong>Em qual camada do Medallion Architecture voc√™ deve remover duplicatas?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>Bronze</li>
                    <li>Silver</li>
                    <li>Gold</li>
                    <li>Qualquer camada</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: B) Silver</strong><br>
                    <p style="margin-top: 10px;">Silver √© a camada de limpeza e valida√ß√£o. Bronze mant√©m dados raw (sem transforma√ß√µes), Gold tem agrega√ß√µes de neg√≥cio.</p>
                </div>

                <h3>Quest√£o 10: Unity Catalog</h3>
                <p><strong>Qual permiss√£o √© SEMPRE necess√°ria antes de qualquer outra no Unity Catalog?</strong></p>
                <ol style="margin: 10px 0 10px 30px;">
                    <li>SELECT</li>
                    <li>CREATE</li>
                    <li>USAGE</li>
                    <li>MODIFY</li>
                </ol>
                <div class="success">
                    <strong>‚úÖ Resposta: C) USAGE</strong><br>
                    <p style="margin-top: 10px;">USAGE √© sempre necess√°rio primeiro em catalog e schema antes de qualquer outra permiss√£o. Sem USAGE, outras permiss√µes n√£o funcionam.</p>
                </div>
            </div>

            <!-- Checklist Final -->
            <div class="section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none;">
                <h2 style="color: white;">üéØ Checklist Final de Prepara√ß√£o</h2>
                
                <div class="comparison-grid">
                    <div class="card" style="background: rgba(255,255,255,0.1); color: white;">
                        <h3>Section 1 (15%)</h3>
                        <ul style="line-height: 2;">
                            <li>‚úì OPTIMIZE e Z-ORDERING</li>
                            <li>‚úì VACUUM e AUTO OPTIMIZE</li>
                            <li>‚úì Tipos de compute</li>
                            <li>‚úì Control vs Data Plane</li>
                        </ul>
                    </div>

                    <div class="card" style="background: rgba(255,255,255,0.1); color: white;">
                        <h3>Section 2 (20%)</h3>
                        <ul style="line-height: 2;">
                            <li>‚úì CDC vs Auto Loader vs COPY INTO</li>
                            <li>‚úì Auto Loader (cloudFiles)</li>
                            <li>‚úì Schema Evolution</li>
                            <li>‚úì Notebooks e Databricks Connect</li>
                        </ul>
                    </div>

                    <div class="card" style="background: rgba(255,255,255,0.1); color: white;">
                        <h3>Section 3 (30%) üî•</h3>
                        <ul style="line-height: 2;">
                            <li>‚úì Medallion Architecture</li>
                            <li>‚úì DLT / Lakeflow Pipelines</li>
                            <li>‚úì SQL DDL/DML (MERGE)</li>
                            <li>‚úì SCD Type 2</li>
                            <li>‚úì PySpark (count_distinct!)</li>
                        </ul>
                    </div>

                    <div class="card" style="background: rgba(255,255,255,0.1); color: white;">
                        <h3>Section 4 (20%)</h3>
                        <ul style="line-height: 2;">
                            <li>‚úì DAB (Asset Bundles)</li>
                            <li>‚úì Workflows (Repair vs Rerun)</li>
                            <li>‚úì Serverless</li>
                            <li>‚úì Spark UI</li>
                        </ul>
                    </div>

                    <div class="card" style="background: rgba(255,255,255,0.1); color: white;">
                        <h3>Section 5 (15%)</h3>
                        <ul style="line-height: 2;">
                            <li>‚úì Managed vs External</li>
                            <li>‚úì GRANT permissions (USAGE!)</li>
                            <li>‚úì Delta Sharing</li>
                            <li>‚úì Audit logs</li>
                        </ul>
                    </div>
                </div>

                <div style="background: rgba(255,255,255,0.2); padding: 20px; border-radius: 10px; margin-top: 30px;">
                    <h3 style="color: white; margin-top: 0;">üöÄ Pr√≥ximos Passos</h3>
                    <ol style="line-height: 2.5; font-size: 16px;">
                        <li><strong>Estude este guia:</strong> Se√ß√£o por se√ß√£o, come√ßando pela Section 3 (30%)</li>
                        <li><strong>Fa√ßa os m√≥dulos Partners:</strong> 8h no total, todos os 4 m√≥dulos</li>
                        <li><strong>Hands-on no Databricks:</strong> Pratique Auto Loader, DLT, Unity Catalog</li>
                        <li><strong>Revise quest√µes:</strong> As quest√µes pr√°ticas deste guia</li>
                        <li><strong>Simulados:</strong> Se poss√≠vel, fa√ßa practice tests</li>
                        <li><strong>Agende a prova:</strong> Quando estiver confiante (70%+ de acerto esperado)</li>
                    </ol>
                </div>

                <div style="text-align: center; margin-top: 30px; font-size: 24px;">
                    üí™ Voc√™ consegue!
                </div>
            </div>
        </div>

        <footer>
            <h3>üë®üíª Sobre o Autor</h3>
            <p><strong>Esdras Rocha</strong></p>
            <p>Engenheiro de Dados S√™nior</p>
            <p>Databricks | ADF | PySpark | Multi-Cloud (AWS & Azure)</p>
            <p style="margin-top: 20px; font-size: 0.9em;">
                Mais de 14 anos transformando dados em valor estrat√©gico, com atua√ß√£o em projetos<br>
                multinacionais focados em arquitetura Lakehouse, pipelines escal√°veis e governan√ßa de dados.
            </p>
            <p style="margin-top: 20px; font-size: 0.8em; opacity: 0.8;">
                Este guia expandido combina o material original com deep dives detalhados em CDC, SCD,<br>
                Auto Loader, COPY INTO, CTAS, OPTIMIZE, Z-ORDER, VACUUM e AUTO OPTIMIZE,<br>
                al√©m de quest√µes pr√°ticas de prova para facilitar sua prepara√ß√£o.
            </p>
        </footer>
    </div>
</body>
</html>
